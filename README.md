
# FAEclust: Cluster Analysis of Multi-Dimensional Functional Data through Non-linear Representation Learning

We introduce **FAEclust**, a functional autoencoder framework for clustering multi-dimensional functional data (vector‚Äêvalued random functions). Our key contributions include:

* A **functional encoder** capturing complex nonlinear dependencies across component functions.
* A **universal‚Äêapproximator decoder** reconstructing both Euclidean and manifold‚Äêvalued functional data.
* **Regularization** strategies on functional weights/biases for stability and robustness.
* A **clustering loss** integrated into training to shape latent representations for clustering, with a **shape‚Äêinformed** objective to resist phase variations and time warping.
* A theoretical proof of the decoder‚Äôs universal approximation property, and extensive empirical validation, are provided in the main paper.

---

<!--
## üì¶ Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/_/FAEclust.git
   cd FAEclust
   ```
2. Create a Python environment and install dependencies:

   ```bash
   python3 -m venv FAE
   source FAE/bin/activate
   pip install -r requirements.txt
   ```

---
-->

## üõ†Ô∏è Core API

FAEclust exposes four main stages:

1. **Smoothing & Basis Construction**
2. **Distance Computation**
3. **Graph Construction**
4. **Functional Autoencoder Training & Clustering**

All modules are available at the package level:

```python
from FAEclust import (
    smoothing_features,    # functional smoothing
    bspline_basis,         # B-spline basis functions
    rescale,               # min‚Äìmax scaling & label encoding
    TimeSeriesDistance,    # DTW / elastic distance
    NearestNeighborsOpt,   # m-NN graph & similarity
    FunctionalAutoencoder  # the joint FAE + clustering model
)
```

---

### 1. Smoothing & Basis Construction (preprocessing)

* **`smoothing_features(data, m, dis_p, fit='bspline')`**
  Smooth each feature dimension using the specified basis and return:

  * `coeffs`: array of shape `(n_samples, p, m)`‚Äîbasis coefficients
  * `curves`: smoothed curves evaluated on the coarse grid
  * `basis_smoothing`: list of smoothing basis functions


* **`bspline_basis(num_basis, degree=3)`**
  Generate a list of `BSpline` basis callables for functional‚Äêweight inputs.


* **`rescale(X, y, name)`**
  Standardize `X` and encode labels `y` as zero‚Äêbased integers.

---

### 2. Distance Computation

* **`TimeSeriesDistance(X, metric='elastic', n_jobs=-1)`**
  Compute pairwise distances for multivariate time series:

  * `'fastdtw'`: standard DTW
  * `'elastic'`: SRVF-based elastic amplitude distance
    Methods:
  * `compute_distances()` ‚Üí returns `(n_samples, n_samples)` distance matrix


---

### 3. Graph Construction

* **`NearestNeighborsOpt(dist_matrix)`**
  Determine an optimal nearest neighbor count `m` via:

  1. **Connectivity**: first `m` yielding one connected component
  2. **Avg-distance knee**: knee in mean m-th neighbor distance curve
     Methods:

  * `estimate_optimal_m(method='avg_distance' or 'connectivity')`
  * `get_nearest_neighbors(opt_m)` ‚Üí dict of neighbor indices
  * `compute_similarity(neighbors_dict)` ‚Üí dense similarity matrix


---

### 4. Functional Autoencoder & Clustering

#### `FunctionalAutoencoder` constructor parameters :

| Argument              | Description                                                                                                                    |
| --------------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| `p` (int)             | Input functional dimension.                                                                 |
| `layers` (list\[int]) | `[q‚ÇÅ, ‚Ä¶, s, ‚Ä¶, Q‚ÇÅ, Z‚ÇÅ, Z‚ÇÇ]` specifying: first encoder and last three decoder functional‚Äêweight counts (`q‚ÇÅ`,`Q‚ÇÅ`,`Z‚ÇÅ`,`Z‚ÇÇ`), with MLP  layer sizes in‚Äêbetween (` ‚Ä¶, s, ‚Ä¶`). |
| `l_basis` (int)             | Number of basis functions for **encoder** expansion.                                                                           |
| `m_basis` (int)             | Number of basis functions for **smoothing** expansion.                                                                         |
| `basis_smoothing`     | List of smoothing basis callables from `smoothing_features()`.                                                                 |
| `basis_input`         | List of input basis callables from `bspline_basis()`.                                                                          |
| `lambda_e` (float)    | Weight for encoder L‚ÇÇ orthonormality penalty.                                                                                  |
| `lambda_d` (float)    | Weight for decoder L‚ÇÅ sparsity penalty.                                                                                        |
| `lambda_c` (float)    | Weight for latent‚Äêspace clustering loss.                                                                                       |
| `t` (array)           | Time grid over which functional data are defined.                                                                              |
| `sim_matrix`          | Precomputed similarity matrix for convex clustering.                                                                           |

#### Training & Inference

```python
# model setup
fae = FunctionalAutoencoder(
    p, layers,
    l=l_basis, m=m_basis,
    basis_smoothing=basis_smoothing,
    basis_input=basis_input,
    lambda_e=Œª_e, lambda_d=Œª_d, lambda_c=Œª_c,
    t=t, sim_matrix=sim_matrix
)
fae.model_summary()

# training
fae.train(
    X_train=coeffs,           # from smoothing_features()
    epochs=E,                 # e.g. 100
    learning_rate=LR,         # e.g. 1e-3
    batch_size=B,             # e.g. 16
    neighbors_dict=neighbors, # from NearestNeighborsOpt
    sim_matrix=sim_matrix
)

# clustering & evaluation
S, pred_labels = fae.predict(coeffs, batch_size=B)
```

---

## üß© Modular Pipeline

The modular pipeline/framework for FAEclust comprises the following stages:

![Modular Pipeline Diagram](modular_pipeline.png)

1. **Raw Dataset**: N samples of p-dimensional multivariate functional data.
2. **Smoothing & Basis Construction**: Convert raw curves into basis functions and coefficients via `smoothing_features()`.
3. **Distance Computation**: Compute pairwise distances (e.g., DTW, elastic) using `TimeSeriesDistance`.
4. **Graph Construction**: Build an m-nearest-neighbor graph and similarity matrix via `NearestNeighborsOpt`.
5. **Functional Autoencoder**: Encode coefficients into a latent vector space and decode back to functional space, balancing reconstruction and clustering objectives.
6. **Convex Clustering**: Apply clustering loss in latent space to obtain final cluster assignments.

---

## üèóÔ∏è Model Framework Schematic

The overall model framework integrates the autoencoder network with clustering updates in an joint optimization scheme:

![Model Framework Schematic](framework.png)

* **Finite-dimensional Functional Space**: Representation via basis expansion of input functions.
* **Encoder (‚Ñá)**: Projects functional input into a low-dimensional latent vector space.
* **Decoder (ùíü)**: Reconstructs functional output from latent vectors, ensuring expressive power.
* **Loss Components**:

  * **Reconstruction Loss**: Measures discrepancy between input and decoder output.
  * **Clustering Loss**: Encourages latent embeddings to form well-separated clusters (convex clustering in latent space).
  * **Regularization Penalties**: L‚ÇÇ orthonormality for encoder weights and L‚ÇÅ sparsity for decoder weights.
* **Training Loop**:

  1. **Network Update**: Backpropagate combined loss to update encoder/decoder weights.
  2. **Cluster Update**: Update cluster centroids/assignments based on current latent representations.

---

## üöÄ Usage Example: 

### 1. UCR ‚ÄúPlane‚Äù Dataset

[üîó View the full notebook](test.ipynb)

### 2. Simulated ‚ÄúPendulum‚Äù Dataset

[üîó View the full notebook](simulation.ipynb)

---
